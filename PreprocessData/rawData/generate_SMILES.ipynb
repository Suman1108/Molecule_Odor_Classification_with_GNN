{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e710778",
   "metadata": {},
   "source": [
    "#### Generate SMILES using API: https://cactus.nci.nih.gov/chemical/structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0030d6",
   "metadata": {},
   "source": [
    "Import relavent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9681a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148fa460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAS numbers to process: 3839\n",
      "Fetching SMILES using multithreading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3839/3839 [02:22<00:00, 26.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def cas_to_smiles(cas_number):\n",
    "    url = f\"https://cactus.nci.nih.gov/chemical/structure/{cas_number}/smiles\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        smiles = response.text.strip()\n",
    "        return cas_number, smiles if smiles else None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return cas_number, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(\"C:/Users/suman/OneDrive/Bureau/Internship_Study/GNN_On_OdorPrediction/data/raw_data/Odor_with_Smiles.csv\")\n",
    "\n",
    "    if \"cas_number\" not in df.columns:\n",
    "        raise ValueError(\"Column 'cas_number' not found in the CSV file.\")\n",
    "\n",
    "    df = df.dropna(subset=[\"cas_number\"])\n",
    "\n",
    "    # Get CAS numbers\n",
    "    unique_cas_numbers = df[\"cas_number\"].unique()\n",
    "    print(\"CAS numbers to process:\", len(unique_cas_numbers))\n",
    "\n",
    "    # Fetch SMILES in parallel\n",
    "    print(\"Fetching SMILES using multithreading...\")\n",
    "    cas_to_smiles_dict = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(cas_to_smiles, cas): cas for cas in unique_cas_numbers}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            cas, smiles = future.result()\n",
    "            if smiles:  # Store only valid results\n",
    "                cas_to_smiles_dict[cas] = smiles\n",
    "\n",
    "    # Map SMILES to all rows (including duplicates)\n",
    "    df[\"SMILES\"] = df[\"cas_number\"].map(cas_to_smiles_dict)\n",
    "\n",
    "    # Reorder columns: CAS + SMILES first\n",
    "    first_cols = [\"cas_number\", \"SMILES\"]\n",
    "    other_cols = [col for col in df.columns if col not in first_cols]\n",
    "    df = df[first_cols + other_cols]\n",
    "\n",
    "    # Save result\n",
    "    df.to_csv(\"Odor_SMILES.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806128a",
   "metadata": {},
   "source": [
    "Since the SMILES generation may be interrupted due to API rate limits, we handle this by splitting the data. Initially, we generate SMILES for available CAS numbers and split the output into two parts: `Odor_SMILE_Part1` (with successfully generated SMILES) and `Part2` (with missing SMILES). We then retry `Part2` through the same process, producing a new file that we again split. The newly generated SMILES (`Part3`) are concatenated with `Part1`, and the cycle continues until the API stops returning results entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f832f",
   "metadata": {},
   "source": [
    "#### **Split** CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6462df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"/content/OdorSmiles_Updated.csv\", dtype={'cas_number': str})\n",
    "\n",
    "# Split row\n",
    "split_row = # Until the SMILES have been fetched \n",
    "\n",
    "# Keep header + row 0 in both parts\n",
    "df_part1 = df.iloc[:split_row]\n",
    "df_part2 = pd.concat([df.iloc[[0]], df.iloc[split_row:]], ignore_index=True)\n",
    "\n",
    "# Save the parts\n",
    "df_part1.to_csv(\"Odor_SMILES_part1.csv\", index=False)\n",
    "df_part2.to_csv(\"Odor_SMILES_part2.csv\", index=False)\n",
    "\n",
    "print(f\"CSV file split at row {split_row}, keeping the first row in both parts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ba816",
   "metadata": {},
   "source": [
    "#### Concate 2 CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both CSV files\n",
    "df1 = pd.read_csv(\"Inital CSV with smiles\")\n",
    "df2 = pd.read_csv(\"newly generated csv from Odor_SMILES_part2.csv\")\n",
    "\n",
    "# Concatenate them as rows\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save to a new file\n",
    "merged_df.to_csv(\"Odor_SMILES_merged12.csv\", index=False)\n",
    "\n",
    "print(\"Files merged successfully into 'Odor_SMILES_merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1bb3b",
   "metadata": {},
   "source": [
    "More than 2000 SMILES were successfully retrieved from CAS numbers using the CACTUS API. For the remaining CAS values where CACTUS did not return results, we used the PubChemPy API to complete the SMILES extraction. This two-step approach ensured maximum coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pubchempy as pcp\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CSV\n",
    "input_file = \"C:/Users/suman/OneDrive/Bureau/Internship_Study/GNN_On_OdorPrediction/data/raw_data/OdorCAS_preprocessed.csv\"\n",
    "output_file = \"C:/Users/suman/OneDrive/Bureau/Internship_Study/GNN_On_OdorPrediction/data/raw_data/Odor_with_Smiles.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Detect CAS column\n",
    "cas_column = \"cas_number\" if \"cas_number\" in df.columns else df.columns[0]\n",
    "\n",
    "# Cache\n",
    "smiles_cache = {}\n",
    "\n",
    "# Function to fetch SMILES\n",
    "def fetch_smiles(cas):\n",
    "    if cas in smiles_cache:\n",
    "        return smiles_cache[cas]\n",
    "\n",
    "    try:\n",
    "        if not cas or cas.strip() == \"0\":\n",
    "            smiles_cache[cas] = None\n",
    "            return None\n",
    "\n",
    "        cids = pcp.get_cids(cas, namespace='name')\n",
    "        time.sleep(0.2)\n",
    "        if not cids:\n",
    "            print(f\"CAS not found: {cas}\")\n",
    "            smiles_cache[cas] = None\n",
    "            return None\n",
    "\n",
    "        props = pcp.get_properties('SMILES', cids[:1], as_dataframe=False)\n",
    "        smiles = props[0].get('SMILES') if props else None\n",
    "        smiles_cache[cas] = smiles\n",
    "        return smiles\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {cas}: {e}\")\n",
    "        smiles_cache[cas] = None\n",
    "        return None\n",
    "\n",
    "# Add progress bar to apply\n",
    "tqdm.pandas(desc=\"Fetching SMILES\")\n",
    "df['SMILES'] = df[cas_column].astype(str).progress_apply(fetch_smiles)\n",
    "\n",
    "# Reorder columns\n",
    "df = df[[cas_column, 'SMILES'] + [col for col in df.columns if col not in [cas_column, 'SMILES']]]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('C:/Users/suman/OneDrive/Bureau/Internship_Study/GNN_On_OdorPrediction/data/OdorSmiles_Updated.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Drop non-label columns\n",
    "labels_df = df.drop(columns=['SMILES', 'cas_number'])\n",
    "\n",
    "# Filter: keep only odor descriptors with >10 positives\n",
    "descriptor_counts = labels_df.sum(axis=0)\n",
    "valid_descriptors = descriptor_counts[descriptor_counts > 10].index\n",
    "filtered_labels_df = labels_df[valid_descriptors]\n",
    "print(len(valid_descriptors))\n",
    "\n",
    "# Count positive and negative labels\n",
    "positive_counts = filtered_labels_df.sum(axis=0)\n",
    "total_molecules = len(filtered_labels_df)\n",
    "negative_counts = total_molecules - positive_counts\n",
    "\n",
    "# formatted DataFrame\n",
    "label_counts_df = pd.DataFrame({\n",
    "    'Odor Descriptor': positive_counts.index,\n",
    "    'Positive Count': positive_counts.values,\n",
    "    'Negative Count': negative_counts.values\n",
    "})\n",
    "\n",
    "# Sort by positive count\n",
    "label_counts_df = label_counts_df.sort_values(by='Positive Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save to text file\n",
    "output_path = \"positive_negative_label_counts.txt\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"Positive & Negative Label Counts per Odor Descriptor\\n\\n\")\n",
    "    f.write(label_counts_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nSaved summary to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
